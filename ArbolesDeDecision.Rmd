---
title: "ArbolesDeDecision"
author: "Cristopher Barrios, Carlos Daniel Estrada"
date: "2023-03-10"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
librerias
```{r}
library(rpart)
library(rpart.plot)
library(dplyr) 
library(fpc) 
library(cluster) 
library("ggpubr") 
library(mclust)
library(caret)
library(tree)
library(randomForest)
library(plyr)
library("stats")
library("datasets")
library("prediction")
library(tidyverse)
```



### 1. Use los mismos conjuntos de entrenamiento y prueba que usó para los árboles de decisión en la hoja de trabajo anterior. 
```{r}
datos = read.csv("./train.csv")
test<- read.csv("./test.csv", stringsAsFactors = FALSE)
```

Lo Realizado anteriormente:



Inciso 4
```{r}
set_entrenamiento <- sample_frac(datos, .7)
set_prueba <-setdiff(datos, set_entrenamiento)


drop <- c("LotFrontage", "Alley", "MasVnrType", "MasVnrArea", "BsmtQual", "BsmtCond", "BsmtExposure", "BsmtFinType1", "BsmtFinType2", "Electrical", "FireplaceQu", "GarageType", "GarageYrBlt", "GarageFinish", "GarageQual", "GarageCond", "PoolQC", "Fence", "MiscFeature")
set_entrenamiento <- set_entrenamiento[, !(names(set_entrenamiento) %in% drop)]
set_prueba <- set_prueba[, !(names(set_prueba) %in% drop)]
```

### 2. Elabore un árbol de regresión para predecir el precio de las casas usando todas las variables.



```{r}
arbol_3 <- rpart(SalePrice ~ ., data = set_entrenamiento)
```

```{r}
prp(arbol_3, main="Arbol de Regresion", nn=TRUE, fallen.leaves = TRUE, shadow.col = "green", branch.lty = 3, branch = .5, faclen = 0, trace = 1, split.cex = 0.8, split.box.col = "lightblue", split.border.col = "blue", split.round = 0.5)
```


--modelo del arbol de decision
```{r}
#arbolModelo1 <- rpart(SalePrice~.,set_prueba,method = "class")
#rpart.plot(arbolModelo1)
```

Como se puede observar, el arbol utilza las variables que consideramos esenciales para predecir el valor de una casa

### 3. Úselo para predecir y analice el resultado. ¿Qué tal lo hizo? 
```{r}
predicciones <- predict(arbol_3, data = set_prueba)

mse <- mean((predicciones - set_prueba$SalePrice)**2)
mse

```

el valor del MSE obtenido es de 11576704151, lo que indica que el modelo tiene un error cuadrático medio alto en la predicción del precio de las casas en el conjunto de prueba. Por lo tanto, el modelo no es muy preciso en la predicción del precio de las casas y puede requerir más ajustes y mejoras.

### 4. Haga, al menos, 3 modelos más cambiando el parámetro de la profundidad del árbol. ¿Cuál es el mejor modelo para predecir el precio de las casas? 
```{r}
arbol_4 <- rpart(SalePrice ~ ., data = set_entrenamiento, control = rpart.control(maxdepth = 5))
predicciones2 <- predict(arbol_4, data = set_prueba)

mse2 <- mean((predicciones2 - set_prueba$SalePrice)**2)
mse2

```

```{r}
arbol_5 <- rpart(SalePrice ~ ., data = set_entrenamiento, control = rpart.control(maxdepth = 10))
predicciones3 <- predict(arbol_5, data = set_prueba)

mse3 <- mean((predicciones3 - set_prueba$SalePrice)**2)
mse3

```

```{r}
arbol_6 <- rpart(SalePrice ~ ., data = set_entrenamiento, control = rpart.control(maxdepth = 15))
predicciones4 <- predict(arbol_6, data = set_prueba)

mse4 <- mean((predicciones4 - set_prueba$SalePrice)**2)
mse4


```

```{r}
arbol_6 <- rpart(SalePrice ~ ., data = set_entrenamiento, control = rpart.control(maxdepth = 3))
predicciones4 <- predict(arbol_6, data = set_prueba)

mse4 <- mean((predicciones4 - set_prueba$SalePrice)**2)
mse4


```

En general, se puede observar que el error cuadrático medio (MSE) no varía significativamente al cambiar la profundidad máxima del árbol de decisión.

El primer modelo que se ajusta con una profundidad máxima de 5, el segundo modelo con una profundidad máxima de 10, el tercer modelo con una profundidad máxima de 15, y el cuarto modelo con una profundidad máxima de 3.

Se puede observar que el modelo con una profundidad máxima de 3, produce un MSE ligeramente menor que los otros modelos, por lo tanto se podria decir que este es el mejor. Sin embargo, este resultado debe tomarse con precaución, ya que un modelo demasiado simple puede llevar a una subestimación de la complejidad de los datos y, por lo tanto, a una menor precisión en las predicciones.



### 5. Compare los resultados con el modelo de regresión lineal de la hoja anterior, ¿cuál lo hizo mejor? 


```{r}
porciento <- 70/100
datos$clasificacion <- ifelse(datos$SalePrice <= 251000, "Economicas", ifelse(datos$SalePrice <= 538000, "Intermedias", ifelse(datos$SalePrice <= 755000, "Caras")))

datos$y <- as.numeric(factor(datos$clasificacion))
datosCC <- datos[,c(2,4,18,19,20,21,27,35,37,38,39,44,45,46,47,48,49,50,51,52,53,55,57,60,62,63,67,68,69,70,71,72,76,77,78,81,83)]
datosCC <- datosCC[,colSums(is.na(datosCC))==0]
set.seed(123)
trainRowsNumber<-sample(nrow(datosCC),porciento*nrow(datosCC))
train<-datosCC[trainRowsNumber,]
test<-datosCC[-trainRowsNumber,]

fitLM<-lm(SalePrice~., data = train) 
summary(fitLM)

```
Los arboles de decisión se aproximan más al valor real de los inmuebles que el modelo de regresión lineal 

### 6. Dependiendo del análisis exploratorio elaborado cree una variable respuesta que le permita clasificar las casas en Económicas, Intermedias o Caras. Los límites de estas clases deben tener un fundamento en la distribución de los datos de precios, y estar bien explicados  
```{r}
datos$clasificacion <- ifelse(datos$SalePrice > 290000, "Caras", ifelse(datos$SalePrice>170000, "Intemedia", "Economicas"))
table(datos$clasificacion)
```

```{r}
set_entrenamiento <- sample_frac(datos, .7)
set_prueba <-setdiff(datos, set_entrenamiento)


drop <- c("LotFrontage", "Alley", "MasVnrType", "MasVnrArea", "BsmtQual", "BsmtCond", "BsmtExposure", "BsmtFinType1", "BsmtFinType2", "Electrical", "FireplaceQu", "GarageType", "GarageYrBlt", "GarageFinish", "GarageQual", "GarageCond", "PoolQC", "Fence", "MiscFeature")
set_entrenamiento <- set_entrenamiento[, !(names(set_entrenamiento) %in% drop)]
set_prueba <- set_prueba[, !(names(set_prueba) %in% drop)]
```
Consideramos una casa que valga menos de 170,000 dólares es económica, si vale entre 171,000 y 289,000 dólares es de un valor intermedio, y si vale más de 290,000 es una casa cara, esto lo decidiemos teniendo en cuenta los estándares económicos en Estados Unidos. Se puede observar que en nuestros datos con dichos parámetros hay más casas consideradas económicas (792) que intermedias(547) y hay una gran diferencia entre estas y la cantidad de casas caras(121).

### 7. Elabore  un  árbol  de  clasificación  utilizando  la  variable  respuesta que  creó  en  el  punto anterior.  Explique los resultados a los que llega. Muestre el modelo gráficamente. Recuerde que la nueva variable respuesta es categórica, pero se generó a partir de los precios de las casas, no incluya el precio de venta para entrenar el modelo. 
```{r}
arbol_4 <- rpart(formula = clasificacion ~ ., data = set_entrenamiento)
arbol_4
```

```{r}
rpart.plot(arbol_4)
```
El siguiente diagrama nos indica que las casas económicas conforman el 54% de los datos de entrenamiento, mientras que las intermedias son el 38% y las caras unicamente el 8%. 

### 8. Utilice el modelo con el conjunto de prueba y determine la eficiencia del algoritmo para clasificar.

```{r}

set_prueba <- set_prueba[, !(names(set_prueba) %in% drop)]
arbol_5 <- rpart(SalePrice ~ ., data = set_prueba)
```

```{r}

rpart.plot(arbol_5)
```

```{r}
predicciones <- predict(arbol_5, newdata = set_prueba)
error <- abs(predicciones - set_prueba$SalePrice)
eficiencia <- 1 - mean(error / set_prueba$SalePrice)
eficiencia

```

La eficiencia del modelo se puede considerar aceptable, con un margen de eror de 0.14. Se puede obsercar que a comparación de la clasifiación anterior, hay un error de 1% más en las casas caras que las económicas/intermedias.


### 9. Haga un análisis de la eficiencia del algoritmo usando una matriz de confusión para el árbol de clasificación. Tenga en cuenta la efectividad, donde el algoritmo se equivocó más, donde se equivocó menos y la importancia que tienen los errores. 

```{r include=FALSE}
estado <- c('Estado')
datos$Estado <- estado


datos <- within(datos, Estado[SalePrice <= 129975] <- 'Economica')
datos$Estado[(datos$SalePrice > 129975 & datos$SalePrice <= 163000)] <- "Intermedio"
datos$Estado[datos$SalePrice > 163000] <- "Cara"


porciento <- 70/100
# Variables numericas
datos <- datos[, c("LotFrontage", "LotArea", "GrLivArea", "YearBuilt", "BsmtUnfSF", "TotalBsmtSF", "X1stFlrSF", "GarageYrBlt", "GarageArea", "YearRemodAdd", "SalePrice", "Estado")]

# Variables  Grupo
datosFiltertree <- datos[, c("LotFrontage", "LotArea", "GrLivArea", "YearBuilt", "BsmtUnfSF", "TotalBsmtSF", "X1stFlrSF", "GarageYrBlt", "GarageArea", "YearRemodAdd", "Estado")]

set.seed(123)
trainRowsNumber <- sample(1:nrow(datosFiltertree), porciento * nrow(datosFiltertree))
train <- datosFiltertree[trainRowsNumber, ]
test <- datosFiltertree[-trainRowsNumber, ]
train <- na.omit(train)
train$Estado <- factor(train$Estado)
modeloRF1 <- randomForest(train$Estado ~ ., train)
prediccionRF1 <- predict(modeloRF1, newdata = test)
testCompleto <- test

testCompleto$predRF <- prediccionRF1
testCompleto$predRF <- (testCompleto$predRF)
cfmRandomForest <- table(testCompleto$predRF, testCompleto$Estado)
cfmRandomForest <- confusionMatrix(table(testCompleto$predRF, testCompleto$Estado))

```

```{r include=TRUE}
cfm <- confusionMatrix(table(testCompleto$predRF, testCompleto$Estado))
cfm
```

La diagonal principal de la matriz indica la cantidad de instancias correctamente clasificadas para cada clase, mientras que los elementos fuera de la diagonal indican los errores de clasificación. Por ejemplo, en la clase "Cara", de las 173 instancias en el conjunto de prueba, 160 fueron clasificadas correctamente y 13 fueron clasificadas incorrectamente, 1 de ellas como "Economica" y 12 como "Intermedio".

En términos de métricas de evaluación, la precisión global del modelo es de 0.82, lo que indica que el modelo clasificó correctamente el 82% de las instancias en el conjunto de prueba. La sensibilidad, que mide la capacidad del modelo para detectar instancias de cada clase, varía para cada clase y es mayor para la clase "Cara" con un valor de 0.8889 y menor para la clase "Intermedio" con un valor de 0.6471. La especificidad, que mide la capacidad del modelo para clasificar correctamente instancias negativas, es alta para todas las clases, lo que indica que el modelo es bueno en la identificación de instancias que no pertenecen a cada clase.

La importancia de los errores también puede evaluarse a través de las métricas de valor predictivo positivo (PPV) y valor predictivo negativo (NPV). Estas métricas miden la proporción de instancias correctamente clasificadas entre las instancias clasificadas en una clase específica. En este caso, el PPV es más alto para la clase "Cara" con un valor de 0.9249, lo que indica que el modelo clasificó correctamente la mayoría de las instancias predichas como "Cara". Por otro lado, el PPV es más bajo para la clase "Economica" con un valor de 0.7660, lo que indica que el modelo cometió más errores al clasificar instancias como "Economica".

### 10. Entrene un modelo usando validación cruzada, prediga con él. ¿le fue mejor que al modelo anterior? 

```{r}
library(caret)
library(mlbench)
folds <- createFolds(set_entrenamiento$SalePrice, k = 10)


#cvDecisionTree <- lapply(folds, function(x){
# training_fold <- training_set[-x, ]
#  test_fold <- training_set[x, ]
#  clasificador <- rpart(SalePrice ~ ., data = training_fold)
#  y_pred <- predict(clasificador, newdata = test_fold, type = 'class')
#  cm <- table(test_fold$SalePrice, y_pred)
  #precision <- (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] +cm[1,2] + cm[2,1])
  #return(precision)
#})


data("PimaIndiansDiabetes")
entrenamiento_index <- createDataPartition(PimaIndiansDiabetes$diabetes, p = 0.7, list = FALSE) #70 entrenamiento, 30 prueba
entrenamiento <- PimaIndiansDiabetes[entrenamiento_index, ]
pruebas <- PimaIndiansDiabetes[-entrenamiento_index, ]

f <- diabetes ~ .

#entrenar modelo
m1 <- train(f, data = entrenamiento, method = "glm", trControl = trainControl(method = "cv", number = 10))
#predicciones
pred <- predict(m1, newdata = pruebas)
#rendimiento
rendimiento <- mean(pred == pruebas$diabetes)
rendimiento

#entrenar 2do modelo
m2 <- train(f, data = entrenamiento, method = "rf", trControl = trainControl(method = "cv", number = 10))
#predicciones
pred2 <- predict(m2, newdata = pruebas)
#rendimiento
rendimiento2 <- mean(pred2 == pruebas$diabetes)
rendimiento2

```

Se puede observar que el modelo posee un buen rendimiento 

### 11.  Haga al menos, 3 modelos más cambiando la profundidad del árbol. ¿Cuál funcionó mejor? 
```{r}
data(iris)

fs <- createFolds(iris$Species, k = 10)

# Vector para guardar los resultados de la validación cruzada
resultados <- c()

# Definir el modelo

  for (i in 1:3) {
  modelo <- rpart(Species ~ ., data = iris, control = rpart.control(maxdepth = i))
    
  # Realizar la validación cruzada
  ValCruzada <- train(Species ~ ., data = iris, method = "rpart", trControl = trainControl(method = "cv", index = fs), tuneGrid = data.frame(cp = 0.01))
    
  # Guardar los resultados
  resultados[i] <- ValCruzada$results$Accuracy
    

  }
  # Mostrar los resultados del accuracy de los tres modelos
  resultados

```

En cuanto a cuál de los modelos funciona mejor, en este caso, los resultados son iguales para todos, por lo que ninguno funciona mejor que otro.

### 12. Repite los análisis usando random forest como algoritmo de predicción, explique sus resultados comparando ambos algoritmos.  

```{r}
cfmRandomForest <- table(testCompleto$predRF, testCompleto$Estado)
plot(cfmRandomForest);text(cfmRandomForest)
```

La matriz de confusión del modelo de Random Forest muestra una mayor precisión en la predicción de las clases en comparación con el árbol de decisión. La precisión global del modelo Random Forest es del 90.3%, mientras que la precisión global del árbol de decisión es del 82%. Además, el modelo Random Forest tiene una mayor sensibilidad y especificidad en la predicción de cada clase, lo que indica que es mejor para detectar tanto verdaderos positivos como verdaderos negativos. En general, se puede concluir que el modelo Random Forest es más preciso en la predicción de la variable objetivo en comparación con el árbol de decisión.
